	<!DOCTYPE html>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta content="IE=7.0000" http-equiv="X-UA-Compatible">
<title>Wei Ji's Homepage</title>
<meta name="description" content="Wei Ji, Tenure-track Associate Professor in Nanjing University. Computer Vision, Video Understanding, Vision and Language).">
<meta name="keywords" content="Wei Ji, Nanjing University, multi-media, video understanding, deep learning, machine learning">

<link rel="stylesheet" type="text/css" href="./files/weiji.css">
<!-- <script type="text/javascript" async="" src="./files/ga.js"></script><script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-39532305-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script> -->

<style>@-moz-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@-webkit-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@-o-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}embed,object{animation-duration:.001s;-ms-animation-duration:.001s;-moz-animation-duration:.001s;-webkit-animation-duration:.001s;-o-animation-duration:.001s;animation-name:nodeInserted;-ms-animation-name:nodeInserted;-moz-animation-name:nodeInserted;-webkit-animation-name:nodeInserted;-o-animation-name:nodeInserted;}</style></head>


<body>

<div id="content">

<div id="news">
    <h2>News</h2><br>
    <font size="3px">

    <b> Jan 2025</b><br>
    <span class="easylink">
      One full paper is accepted by ICLR, about video moment retrieval. One full paper is accepted by ICRA, about cross-modal generation.
    </span><br><br>
    </font>
	
    <b> Jan 2025</b><br>
    <span class="easylink">
      I will serve as an Area Chair for IJCAI and IJCNN.
    </span><br><br>
    </font>
	    
    <b> Dec 2024</b><br>
    <span class="easylink">
      We are hosting the 1st Workshop on Navigating the Future: Ensuring Trustworthiness in Multi-Modal Open-World Intelligence (CVPR 2025, Nashville TN). Please visit <a href="https://cvpr25workshop.netlify.app/" target="_blank">here</a> for details.
    </span><br><br>
    </font>
	    
    <b> Dec 2024</b><br>
    <span class="easylink">
      One full paper is accepted by AAAI, about visual-audio segmentation.
    </span><br><br>
    </font>
	    
    <b> Nov 2024</b><br>
    <span class="easylink">
      Two full papers are accepted by ACM TOMM, about referring image segmentation, and cloud-device collaboration.
    </span><br><br>
    </font>

    <b> July 2024</b><br>
    <span class="easylink">
      Five full papers are accepted by ACM MM 2024, about image editing, speech event extraction, multi-modal LLM, visual programming, and video tube retrieval.
    </span><br><br>
    </font>
	    
    <b> July 2024</b><br>
    <span class="easylink">
      One full paper is accepted by ECCV 2024, about text-guided geolocalization.
    </span><br><br>
    </font>

	
    <b> May 2024</b><br>
    <span class="easylink">
      We are hosting the 2nd International Workshop on Deep Multimodal Generation and Retrieval (ACM MM2024, Melbourne). Please visit <a href="https://videorelation.nextcenter.org/MMGR24/" target="_blank">here</a> for details.
    </span><br><br>
    </font>

	    
    <b> May 2024</b><br>
    <span class="easylink">
      Three full papers are accepted by ICML 2024, about multi-modal pretrained models (NExT-GPT & NExT-Chat), and multi-modal reasoning.
    </span><br><br>
    </font>
	    
    <b> April 2024</b><br>
    <span class="easylink">
      One full paper is accepted by SIGIR 2024, about dense retrieval.
    </span><br><br>
    </font>

	
    <b> March 2024</b><br>
    <span class="easylink">
      One full paper is accepted by TIP 2024, about video relation detection.
    </span><br><br>
    </font>
	    
	    
    <b> Feb 2024</b><br>
    <span class="easylink">
      One full paper is accepted by CVPR 2024, about text-video generation.
    </span><br><br>
    </font>
	    
	    
    <b> Feb 2024</b><br>
    <span class="easylink">
      We are organizing a Special Issue on ACM TOMM about multi-modal retrieval and generation. Please visit <a href="https://dl.acm.org/pb-assets/static_journal_pages/tomm/pdf/ACM-SI_ToMM_MMGR-1708635711467.pdf" target="_blank">here</a> for details.
    </span><br><br>
    </font>
	    
	    
    <b> Jan 2024</b><br>
    <span class="easylink">
      Three full papers are accepted by ICLR, about LLM fine-tuning, image retrieval, and model selection.
    </span><br><br>
    </font>
	    

	    
    <b> Dec 2023</b><br>
    <span class="easylink">
      One full paper is accepted by AAAI, about scene graph generation. Two full papers are accepted by ICASSP, about scene graph generation and video moment retrieval.
    </span><br><br>
    </font>
	
    <b> Nov 2023</b><br>
    <span class="easylink">
      I will serve as an Area Chair for ACM Multimedia 2024.
    </span><br><br>
    </font>
	    
    <b> Oct 2023</b><br>
    <span class="easylink">
      One full paper is accepted by EMNLP, about video reasoning. One full paper is accepted by TPAMI, about video question answering.
    </span><br><br>
    </font>


	
    <b> Sep 2023</b><br>
    <span class="easylink">
      One full paper is accepted by NeurIPS, about visual prompt generator. 
    </span><br><br>
    </font>
	    
    <b> July 2023</b><br>
    <span class="easylink">
      Four full papers are accepted by ACM MM, about Video Moment Retrieval, Scene Graph Generation, Visual Instance Retrieval, and Multi-modal Recommendation. 
    </span><br><br>
    </font>

	    
    <b> July 2023</b><br>
    <span class="easylink">
      Two full papers are accepted by ICCV, about Scene Graph Generation and Prompt Learning. 
    </span><br><br>
    </font>
	
    <b> May 2023</b><br>
    <span class="easylink">
      Two full papers are accepted by ACL, about Image Captioning and Visual Spatial Description. One paper is accepted by ACL Findings, about video-based fake news detection.
    </span><br><br>
    </font>
	
	
    <b> April 2023</b><br>
    <span class="easylink">
      We are hosting the 1st Deep Multimodal Learning for Information Retrieval (ACM MM2023, Ottawa). Please visit <a href="https://vru-next.github.io/MMIR23/" target="_blank">here</a> for details.
    </span><br><br>
    </font>
	    
    <b> February 2023</b><br>
    <span class="easylink">
     Two full papers are accepted by CVPR, about video moment retrieval and spatial-temporal video grounding.
    </span><br><br>
    </font>
	
	
    <b> November 2022</b><br>
    <span class="easylink">
     Two full papers are accepted by AAAI, about video-audio domain generalization and video-based fake news detection.
    </span><br><br>
    </font>
	
    <b> October 2022</b><br>
    <span class="easylink">
     Two full papers are accepted by EMNLP, about video question answering and pretrained vision-language model.
    </span><br><br>
    </font>	
	    
    <b> July 2022</b><br>
    <span class="easylink">
     One full paper is accepted by ECCV, about scene graph generation.
    </span><br><br>
    </font>
	
	
    <b> May 2022</b><br>
    <span class="easylink">
     One full paper is accepted by TIP, about image super-resolution.
    </span><br><br>
    </font>	 
	
    <b> April 2022</b><br>
    <span class="easylink">
     One full paper is accepted by SIGIR'22, about conversational search.
    </span><br><br>
    </font>	    
	    
    <b> March 2022</b><br>
    <span class="easylink">
     One full paper is accepted by CVPR'22, about video question answering.
    </span><br><br>
    </font>
	
    <b> Dec 2021</b><br>
    <span class="easylink">
     Three full papers are accepted by AAAI'22, about video question answering, grounded situation recognition, and image quality assessment.
    </span><br><br>
    </font>
	    
	    
    <b> August 2021</b><br>
    <span class="easylink">
     One full paper is accepted by ACM MM'21, about video relation detection.
    </span><br><br>
    </font>
	    
    <b> July 2021</b><br>
    <span class="easylink">
     One full paper is accepted by SIGIR'21, about natural language video localization.
    </span><br><br>
    </font>
	
    <b> May 2021</b><br>
    <span class="easylink">
      We are hosting the 3rd Video Relation Understanding Grand Challenge (ACM MM2021, Chengdu). Please visit <a href="https://videorelation.nextcenter.org" target="_blank">here</a> for details.
    </span><br><br>
    </font>
	
    <b> December 2020</b><br>
    <span class="easylink">
      One full paper is accepted by AAAI'21, about natural language video localization.
    </span><br><br>
    </font>

    <b> August 2020</b><br>
    <span class="easylink">
      I have successfully defended my thesis and got the PhD degree! My thesis title is "Research on pixel-level semantic understanding based on deep learning".
    </span><br><br>
    </font>

</div>

<div id="left">
<table style="background-color:white;">
<tbody><tr nosave="">
<td valign="CENTER">
<img src="./images/jiwei-auc-resize.jpg" height="200" align="left">
</td>

<td valign="CENTER" width="2%">
</td>

<td valign="CENTER" halign="LEFT">
<font size="+0">
<b><font size="+2">Wei Ji&nbsp;</font></b>
<!--<p style="margin-left:0px;">
<img src="./images/name.png", height="60">
</p>-->
<p style="margin-left:0px;">
<b>Tenure-track Associate Professor</b>
</p><p style="margin-left:0px;">

<a href="https://is.nju.edu.cn/is_en/main.htm", target="_blank">School of Intelligence Science and Technology</a><br/>
<a href="https://www.nju.edu.cn/en/", target="_blank">Nanjing University</a><br/>

</p><p style="margin-left:0px;">
No. 1520 Taihu Road, Suzhou City, Jiangsu Province, China, 215163<br>
</p><p style="margin-left:0px;">
Email: weiji0523 AT gmail.com</a><br>
</p><p style="margin-left:0px;">
Email: weiji AT nju.edu.cn</a><br>
</p><p style="margin-left:0px;">
<s>Email: jiwei AT nus.edu.sg</a><br></s>
&bull; <a href="https://scholar.google.com/citations?user=69OFB-AAAAAJ&hl=en">Google Scholar</a> <be>
&bull; <a href="https://openreview.net/profile?id=~Wei_Ji1">OpenReview</a> <br>
&bull; <a href="https://twitter.com/weiji0523">Twitter</a> <be>
&bull; <a href="https://is.nju.edu.cn/jw/main.htm">Homepage@NJU</a> <br>
	
</p></font><p><font size="+0">
</font>
</p></td>
</tr>
</tbody></table>

<div style="margin-top:20px;">
    I have joined Nanjing University as a tenure-track associate professor in Fall 2024. I was a senior research fellow in the School of Computing, National University of Singapore, working with <a href="https://www.chuatatseng.com/">Prof. Tat-seng Chua</a> and <a href="https://www.comp.nus.edu.sg/cs/people/rogerz/">Prof. Roger Zimmermann</a>. I obtained my Ph.D degree from <a href="http://www.cs.zju.edu.cn/_upload/article/files/d4/45/e46a2ca6469693738d84d1fffc3f/cc70a9ee-44ce-4603-9e46-701564a0eb2a.pdf">DCD Lab@Zhejiang University</a>, under the supervision of <a href="https://person.zju.edu.cn/en/yzhuang">Prof. Yueting Zhuang</a> and <a href="https://person.zju.edu.cn/en/xilics">Prof. Xi Li</a>. I have published 60+ papers in top conferences such as ICML, NeurIPS, ICLR, CVPR, ECCV, ACM MM, ACL, EMNLP, SIGIR, 
    and journals including TPAMI, TIP and TCYB. Moreover, I have served as Area Chair for ACM MM, Guest Editor for ACM TOMM, and the PC member for top-tier conferences/journals, including NeurIPS, ICLR, ICML, SIGIR, CVPR, ICCV, ECCV, AAAI, EMNLP, TPAMI, IJCV, TIP, TMM, etc.
<br>
	My research interests include but are not limited to: <br>
	&bull;Multi-modal Understanding and Generation;<br>

	&bull;Multi-modal Retrieval and Recommendation;<br>

	&bull;Multi-modal Pretrained Models;<br>

	&bull;Multi-modal Security and Privacy.<br>

    
</div>

<div style="margin-top:20px;">
    I am actively seeking highly motivated Ph.D/Master candidates who share my research interests. Kindly reach out to me at weiji0523@gmail.com with your resume!
</div>




<h2 style="CLEAR: both;">Professional Services</h2>
<table><tbody><tr><td>
<!-- 	Local Chair of <span class="title">CCIS 2019</span> (IEEE International Conference on Cloud Computing and Intelligence Systems) <br> -->
    Local chair of <span class="title">EMNLP'25 </span> <br> 
    Publicity co-chair of <span class="title">ACM MM'25 </span> <br> 
    Associate Editor of <span class="title">ACM TOMM </span> <br> 
    Area Chair of <span class="title">NeurIPS, ACM MM, IJCAI, IJCNN </span> <br>   
    Program Committee Member of <span class="title">NeurIPS, ICML, ICLR, WWW, SIGIR, CVPR, ICCV, ECCV, AAAI, ACM MM, ACL, EMNLP, ACM Multimedia Asia</span> <br>  
    Invited Reviewer for <span class="title">TPAMI, IJCV, TIP, TMM, TCSVT, ACM TOMM </span> <be>
    
</td></tr></tbody></table>


<div id="papers">
<h2 style="CLEAR: both">Workshops & Grand Challenges</h2>
</br>

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://cvpr25workshop.netlify.app/" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">(MM-Open'25) Navigating the Future: Ensuring Trustworthiness in Multi-Modal Open-World Intelligence</span> 
      <br>Wei Ji, Hong Liu, Zhun Zhong, Zhe Zeng, Elisa Ricci, Andrew Gordon Wilson, Shin’ichi Satoh, Nicu Sebe
    <br>CVPR 2025 Workshop&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>
	
<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://videorelation.nextcenter.org/MMGR24/" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">(MMGR'24) The 2nd International Workshop on Deep Multimodal Generation and Retrieval</span> 
      <br>Wei Ji, Hao Fei, Yinwei Wei, Zhedong Zheng, Juncheng Li, Zhiqi Ge, Long Chen, Lizi Liao, Yueting Zhuang, Roger Zimmermann
    <br>ACM MM 2024 Workshop&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://videorelation.nextcenter.org/MMIR23/" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">(MMIR'23) Deep Multimodal Learning for Information Retrieval</span> 
      <br>Wei Ji, Yinwei Wei, Zhedong Zheng, Hao Fei, Tat-seng Chua
    <br>ACM MM 2023 Workshop&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://videorelation.nextcenter.org/" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">(VRU'21) Video Relation Understanding Grand Challenge</span> 
      <br>Wei Ji, Yicong Li, Xindi Shang, Xiaoyu Du, Tongwei Ren, Tat-Seng Chua
    <br>ACM MM 2021 Grand Challenge&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>
	
<!-- =======================================================================!-->



<div id="papers">
<h2 style="CLEAR: both">Selected Publications</h2>
</br>

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2309.05519.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">NExT-GPT: Any-to-any multimodal llm</span> 
      <br>Shengqiong Wu, Hao Fei, Leigang Qu, <b>Wei Ji </b>, Tat-Seng Chua
    <br>ICML 2024 (Oral)&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2311.04498" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">NExT-Chat: An lmm for chat, detection and segmentation</span> 
      <br>Ao Zhang, Yuan Yao, <b>Wei Ji </b>, Zhiyuan Liu, Tat-Seng Chua
    <br>ICML 2024&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://jiwei0523.github.io/" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Backpropogation-Free Multi-modal On-Device Model Adaptation via Cloud-Device Collaboration</span> 
      <br><b>Wei Ji </b>, Li Li, Zheqi Lv, Wenqiao Zhang, Mengze Li, Zhen Wan, Wenqiang Lei, Roger Zimmermann
    <br>ACM TOMM 2024&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>	

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://jiwei0523.github.io/" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Towards Complex-query Referring Image Segmentation: A Novel Benchmark</span> 
      <br><b>Wei Ji </b>, Li Li, Hao Fei, Xiangyan Liu, Xun Yang, Juncheng Li, Roger Zimmermann
    <br>ACM TOMM 2024&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>
	
<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Fei_Dysen-VDM_Empowering_Dynamics-aware_Text-to-Video_Diffusion_with_LLMs_CVPR_2024_paper.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Dysen-VDM: Empowering Dynamics-aware Text-to-Video Diffusion with LLMs</span> 
      <br>Hao Fei, Shengqiong Wu, <b>Wei Ji </b>, Hanwang Zhang, Tat-Seng Chua
    <br>CVPR 2024&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>



<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Fei_Dysen-VDM_Empowering_Dynamics-aware_Text-to-Video_Diffusion_with_LLMs_CVPR_2024_paper.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Video-of-thought: Step-by-step video reasoning from perception to cognition</span> 
      <br>Hao Fei, Shengqiong Wu, <b>Wei Ji </b>, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, Wynne Hsu
    <br>ICML 2024 (Oral)&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>
	

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://dl.acm.org/doi/pdf/10.1145/3664647.3681632" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Hierarchical Debiasing and Noisy Correction for Cross-domain Video Tube Retrieval</span> 
      <br>Jingqiao Xiu, Mengze Li, <b>Wei Ji </b>, Jingyuan Chen, Hanbin Zhao, Shin'ichi Satoh, Roger Zimmermann
    <br>ACM MM 2024&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>



	
	
<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://ieeexplore-ieee-org.libproxy1.nus.edu.sg/abstract/document/10214041/" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Transformer-empowered invariant grounding for video question answering</span> 
      <br>Yicong Li, Xiang Wang, Junbin Xiao, <b>Wei Ji </b>, Tat-Seng Chua
    <br>TPAMI 2023&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

	


	

	
<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2305.01278.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Transfer visual prompt generator across llms</span> 
      <br>Ao Zhang, Hao Fei, Yuan Yao, <b>Wei Ji </b>, Li Li, Zhiyuan Liu, Tat-Seng Chua
    <br>NeurIPS 2023&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

	
<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://jiwei0523.github.io/" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Learning Style-Invariant Robust Representation for Generalizable Visual Instance Retrieval</span> 
      <br>Tianyu Chang, Xun Yang, Xin Luo, <b>Wei Ji </b>, Meng Wang
    <br>ACM MM 2023&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


	
<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://jiwei0523.github.io/" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Partial Annotation-based Video Moment Retrieval via Iterative Learning</span> 
      <br><b>Wei Ji </b>, Renjie Liang, Lizi Liao, Hao Fei, Fuli Feng
    <br>ACM MM 2023&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

	
<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2308.04067.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Online Distillation-enhanced Multi-modal Transformer for Sequential Recommendation</span> 
      <br><b>Wei Ji </b>, Xiangyan Liu, An Zhang, Yinwei Wei, Yongxin Ni, Xiang Wang
    <br>ACM MM 2023&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


	
<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2303.13233.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Visually-Prompted Language Model for Fine-Grained Scene Graph Generation in an Open World.</span> 
      <br>Qifan Yu, Juncheng Li, Yu Wu, Siliang Tang, <b>Wei Ji </b>, Yueting Zhuang
    <br>ICCV 2023&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>	


	
<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2306.05241.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Two Heads Are Better Than One: Improving Fake News Video Detection by Correlating.</span> 
      <br>Peng Qi, Yuyang Zhao, Yufeng Shen, <b>Wei Ji </b>, Juan Cao, Tat-Seng Chua.
    <br>ACL 2023 Findings&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>
	

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2305.11768.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Generating Visual Spatial Description via Holistic 3D Scene Understanding.</span> 
      <br>Yu Zhao, Hao Fei,<b>Wei Ji </b>, Jianguo Wei, Meishan Zhang, Min Zhang, Tat-Seng Chua.
    <br>ACL 2023 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2305.12260.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Cross2StrA: Unpaired Cross-lingual Image Captioning with Cross-lingual Cross-modal Structure-pivoted Alignment.</span> 
      <br>Shengqiong Wu, Hao Fei, <b>Wei Ji</b>, Tat-Seng Chua.
    <br>ACL 2023 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>



<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_WINNER_Weakly-Supervised_hIerarchical_decompositioN_and_aligNment_for_Spatio-tEmporal_Video_gRounding_CVPR_2023_paper.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">WINNER: Weakly-supervised hIerarchical decompositioN and aligNment for spatio-tEmporal video gRounding.</span> 
      <br>Mengze Li, Han Wang, Wenqiao Zhang, Jiaxu Miao,<b>Wei Ji</b>, Zhou Zhao, Shengyu Zhang, Fei Wu.
    <br>CVPR 2023 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ji_Are_Binary_Annotations_Sufficient_Video_Moment_Retrieval_via_Hierarchical_Uncertainty-Based_CVPR_2023_paper.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Are Binary Annotations Sufficient? Video Moment Retrieval via Hierarchical Uncertainty-based Active Learning.</span> 
      <br><b>Wei Ji</b>, Renjie Liang, Zhedong Zheng, Wenqiao Zhang, Shengyu Zhang, Juncheng, Li and Mengze Li, Tat-Seng Chua.
    <br>CVPR 2023 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>



<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2211.10973.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">FakeSV: A Multimodal Benchmark with Rich Social Context for Fake News Detection on Short Video Platforms</span> 
      <br>Peng Qi, Yuyan Bu, Juan Cao, <b>Wei Ji</b>, Ruihao Shui, Junbin Xiao, Danding Wang, Tat-Seng Chua
    <br>AAAI 2023 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2205.11169" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models</span> 
      <br>Yuan Yao, Qianyu Chen, Ao Zhang, <b>Wei Ji</b>, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun
    <br>EMNLP 2022 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2203.01225" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Video Question Answering: Datasets, Algorithms and Challenges</span> 
      <br>Yaoyao Zhong, Junbin Xiao, <b>Wei Ji</b>, Yicong Li, Weihong Deng, Tat-Seng Chua
    <br>EMNLP 2022 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table>
  <tbody>
  <tr>
    <td class="left"><a href="doi.acm.org?doi=3477495.3532063" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Structured and Natural Responses Co-generation for Conversational Search</span> 
      <br>Chenchen Ye, Lizi Liao, Fuli Feng, <b>Wei Ji</b>, Tat-Seng Chua
    <br>SIGIR 2022 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/abs/2203.11654.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Fine-Grained Scene Graph Generation with Data Transfer</span> 
      <br>Ao Zhang, Yuan Yao, Qianyu Chen, <b>Wei Ji</b>, Zhiyuan Liu, Maosong Sun, Tat-Seng Chua
    <br>ECCV 2022 (oral)&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2104.03926.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Conditional Meta-Network for Blind Super-Resolution with Multiple Degradations</span> 
      <br>Guanghao Yin, Wei Wang, Zehuan Yuan, <b>Wei Ji</b>, Dongdong Yu, Shouqian Sun, Tat-Seng Chua, Changhu Wang
    <br>TIP 2022 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Invariant_Grounding_for_Video_Question_Answering_CVPR_2022_paper.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Invariant grounding for video question answering</span> 
      <br>Yicong Li, Xiang Wang, Junbin Xiao, <b>Wei Ji</b>, Tat-Seng Chua
    <br>CVPR 2022 (Oral, Best Paper Finalist) &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2202.13123" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Content-Variant Reference Image Quality Assessment via Knowledge Distillation</span> 
      <br>Guanghao Yin, Wei Wang, Zehuan Yuan, Chuchu Han, <b>Wei Ji</b>, Shouqian Sun, Changhu Wang
    <br>AAAI 2022 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2112.05375" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Rethinking the Two-Stage Framework for Grounded Situation Recognition</span> 
      <br>Meng Wei, Long Chen, <b>Wei Ji</b>, Xiaoyu Yue, Tat-Seng Chua
    <br>AAAI 2022 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2112.06197" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Video as Conditional Graph Hierarchy for Multi-Granular Question Answering</span> 
      <br>Junbin Xiao, Angela Yao, Zhiyuan Liu, Yicong Li, <b>Wei Ji</b>, Tat-Seng Chua
    <br>AAAI 2022 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2105.12694?ref=https://githubhelp.com" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Deep Learning for Weakly-Supervised Object Detection and Localization: A Survey</span> 
      <br>Feifei Shao, Long Chen, Jian Shao, <b>Wei Ji</b>, Shaoning Xiao, Lu Ye, Yueting Zhuang, Jun Xiao
    <br>Neurocomputing &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>



<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://dl.acm.org/doi/pdf/10.1145/3404835.3462823" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Deconfounded Video Moment Retrieval with Causal Intervention</span> 
      <br>Xun Yang, Fuli Feng, <b>Wei Ji</b>, Meng Wang, Tat-Seng Chua
    <br>SIGIR 2021 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://dl.acm.org/doi/pdf/10.1145/3474085.3475263" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Video Visual Relation Detection via Iterative Inference</span> 
      <br>Xindi Shang, Yicong Li, Junbin Xiao, <b>Wei Ji</b>, Tat-Seng Chua
    <br>ACM MM 2021 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://www.aaai.org/AAAI21Papers/AAAI-6267.XiaoS.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Boundary Proposal Network for Two-Stage Natural Language Video Localization</span> 
      <br>Shaoning Xiao, Long Chen, Songyang Zhang, <b>Wei Ji</b>, Jian Shao, Lu Ye, Jun Xiao
    <br>AAAI 2021&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>



<h2 style="CLEAR: both;">Honors</h2>

<table><tbody><tr><td>
  <span class="title">Outstanding Graduate, Zhejiang University&nbsp;&nbsp; 2020</span> &nbsp;&nbsp;
</td></tr></tbody></table>

<table><tbody><tr><td>
  <span class="title">Distinguished Doctoral Scholarship, Zhejiang University&nbsp;&nbsp; 2018-2019</span> &nbsp;&nbsp;
</td></tr></tbody></table>



<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=6IxMkD9AxgkXmtT-CMTMolftstwgqiD1ExJBl3I8mPE&cl=ffffff&w=a"></script>

</div>
</div>



</body></html>
